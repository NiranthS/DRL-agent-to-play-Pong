{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pong_new_new.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jrI6q7RmWQam"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrI6q7RmWQam",
        "colab_type": "text"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"http://introtodeeplearning.com\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/mit.png\" style=\"padding-bottom:5px;\" />\n",
        "      Visit MIT Deep Learning</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/aamini/introtodeeplearning/blob/master/lab3/RL.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/colab.png?v2.0\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/aamini/introtodeeplearning/blob/master/lab3/RL.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
        "</table>\n",
        "\n",
        "# Copyright Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkd375upWYok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2020 MIT 6.S191 Introduction to Deep Learning. All Rights Reserved.\n",
        "# \n",
        "# Licensed under the MIT License. You may not use this file except in compliance\n",
        "# with the License. Use and/or modification of this code outside of 6.S191 must\n",
        "# reference:\n",
        "#\n",
        "# © MIT 6.S191: Introduction to Deep Learning\n",
        "# http://introtodeeplearning.com\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoXYKhfZMHiw",
        "colab_type": "text"
      },
      "source": [
        "# Laboratory 3: Reinforcement Learning\n",
        "\n",
        "Reinforcement learning (RL) is a subset of machine learning which poses learning problems as interactions between agents and environments. It often assumes agents have no prior knowledge of a world, so they must learn to navigate environments by optimizing a reward function. Within an environment, an agent can take certain actions and receive feedback, in the form of positive or negative rewards, with respect to their decision. As such, an agent's feedback loop is somewhat akin to the idea of \"trial and error\", or the manner in which a child might learn to distinguish between \"good\" and \"bad\" actions.\n",
        "\n",
        "In practical terms, our RL agent will interact with the environment by taking an action at each timestep, receiving a corresponding reward, and updating its state according to what it has \"learned\".  \n",
        "\n",
        "![alt text](https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg)\n",
        "\n",
        "While the ultimate goal of reinforcement learning is to teach agents to act in the real, physical world, games provide a convenient proving ground for developing RL algorithms and agents. Games have some properties that make them particularly well suited for RL: \n",
        "\n",
        "1.   In many cases, games have perfectly describable environments. For example, all rules of chess can be formally written and programmed into a chess game simulator;\n",
        "2.   Games are massively parallelizable. Since they do not require running in the real world, simultaneous environments can be run on large data clusters; \n",
        "3.   Simpler scenarios in games enable fast prototyping. This speeds up the development of algorithms that could eventually run in the real-world; and\n",
        "4.   ... Games are fun! \n",
        "\n",
        "In previous labs, we have explored both supervised (with LSTMs, CNNs) and unsupervised / semi-supervised (with VAEs) learning tasks. Reinforcement learning is fundamentally different, in that we are training a deep learning algorithm to govern the actions of our RL agent, that is trying, within its environment, to find the optimal way to achieve a goal. The goal of training an RL agent is to determine the best next step to take to earn the greatest final payoff or return. In this lab, we focus on building a reinforcement learning algorithm to master two different environments with varying complexity. \n",
        "\n",
        "1.   **Cartpole**:   Balance a pole, protruding from a cart, in an upright position by only moving the base left or right. Environment with a low-dimensional observation space.\n",
        "2.   [**Pong**](https://en.wikipedia.org/wiki/Pong): Beat your competitors (whether other AI or humans!) at the game of Pong. Environment with a high-dimensional observation space -- learning directly from raw pixels.\n",
        "\n",
        "Let's get started! First we'll import TensorFlow, the course package, and some dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvdePP-VyVWp",
        "colab_type": "code",
        "outputId": "27dc9f53-9d51-4c3c-ebda-3dc9891ad0c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!apt-get install -y xvfb python-opengl x11-utils > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay scikit-video > /dev/null 2>&1\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import base64, io, time, gym\n",
        "import IPython, functools\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "!pip install mitdeeplearning\n",
        "import mitdeeplearning as mdl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Collecting mitdeeplearning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/3b/b9174b68dc10832356d02a2d83a64b43a24f1762c172754407d22fc8f960/mitdeeplearning-0.1.2.tar.gz (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 36.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (1.18.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (4.28.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from mitdeeplearning) (0.17.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->mitdeeplearning) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->mitdeeplearning) (1.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->mitdeeplearning) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->mitdeeplearning) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->mitdeeplearning) (0.16.0)\n",
            "Building wheels for collected packages: mitdeeplearning\n",
            "  Building wheel for mitdeeplearning (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mitdeeplearning: filename=mitdeeplearning-0.1.2-cp36-none-any.whl size=2114586 sha256=dace1c3b81de7ca815ebba9365240ac3111b10b0c0e41a1e2edcf669793b1240\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/e1/73/5f01c787621d8a3c857f59876c79e304b9b64db9ff5bd61b74\n",
            "Successfully built mitdeeplearning\n",
            "Installing collected packages: mitdeeplearning\n",
            "Successfully installed mitdeeplearning-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4CPKqAoIcWT",
        "colab_type": "code",
        "outputId": "078346ca-ddb5-44bf-e4b7-20829dd10c97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93zCyf_9InwS",
        "colab_type": "code",
        "outputId": "809c1329-36df-4cb3-ce42-f6b64ccbb011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd drive/My Drive"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6JWWi8Lcr7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"Pong-v0\", frameskip=5)\n",
        "env.seed(1); # for reproducibility"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_vVZRr8Q4R_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Define the agent's action function ###\n",
        "\n",
        "# Function that takes observations as input, executes a forward pass through model, \n",
        "#   and outputs a sampled action.\n",
        "# Arguments:\n",
        "#   model: the network that defines our agent\n",
        "#   observation: observation which is fed as input to the model\n",
        "# Returns:\n",
        "#   action: choice of agent action\n",
        "def choose_action(model, observation0):\n",
        "  # add batch dimension to the observation\n",
        "  observation0 = np.expand_dims(observation0, axis=0)\n",
        "\n",
        "  '''TODO: feed the observations through the model to predict the log probabilities of each possible action.'''\n",
        "  logits = model.predict(observation0)\n",
        "  \n",
        "  # pass the log probabilities through a softmax to compute true probabilities\n",
        "  prob_weights = tf.nn.softmax(logits).numpy()\n",
        "  \n",
        "  '''TODO: randomly sample from the prob_weights to pick an action.\n",
        "  Hint: carefully consider the dimensionality of the input probabilities (vector) and the output action (scalar)'''\n",
        "  action = np.random.choice(env.action_space.n, size=1, p=prob_weights[0])[0]\n",
        "\n",
        "  return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tR9uAWcTnkr",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Define the agent's memory\n",
        "\n",
        "Now that we have instantiated the environment and defined the agent network architecture and action function, we are ready to move on to the next step in our RL workflow:\n",
        "1. **Initialize our environment and our agent**: here we will describe the different observations and actions the agent can make in the environemnt.\n",
        "2. **Define our agent's memory**: this will enable the agent to remember its past actions, observations, and rewards.\n",
        "3. **Define the learning algorithm**: this will be used to reinforce the agent's good behaviors and discourage bad behaviors.\n",
        "\n",
        "In reinforcement learning, training occurs alongside the agent's acting in the environment; an *episode* refers to a sequence of actions that ends in some terminal state, such as the pole falling down or the cart crashing. The agent will need to remember all of its observations and actions, such that once an episode ends, it can learn to \"reinforce\" the good actions and punish the undesirable actions via training. Our first step is to define a simple memory buffer that contains the agent's observations, actions, and received rewards from a given episode. \n",
        "\n",
        "**Once again, note the modularity of this memory buffer -- it can and will be applied to other RL tasks as well!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MM6JwXVQ4JG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Agent Memory ###\n",
        "\n",
        "class Memory:\n",
        "  def __init__(self): \n",
        "      self.clear()\n",
        "\n",
        "  # Resets/restarts the memory buffer\n",
        "  def clear(self): \n",
        "      self.observations = []\n",
        "      self.actions = []\n",
        "      self.rewards = []\n",
        "\n",
        "  # Add observations, actions, rewards to memory\n",
        "  def add_to_memory(self, new_observation, new_action, new_reward): \n",
        "      self.observations.append(new_observation)\n",
        "      self.actions.append(new_action)\n",
        "      # TODO: your update code here\n",
        "      self.rewards.append(new_reward)\n",
        "      '''TODO: update the list of rewards with new reward'''\n",
        "      # TODO: your update code here\n",
        "        \n",
        "memory = Memory()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4YhtPaUVj5m",
        "colab_type": "text"
      },
      "source": [
        "## 3.3 Reward function\n",
        "\n",
        "We're almost ready to begin the learning algorithm for our agent! The next step is to compute the rewards of our agent as it acts in the environment. Since we (and the agent) is uncertain about if and when the game or task will end (i.e., when the pole will fall), it is useful to emphasize getting rewards **now** rather than later in the future -- this is the idea of discounting. This is a similar concept to discounting money in the case of interest. ecall from lecture, we use reward discount to give more preference at getting rewards now rather than later in the future. The idea of discounting rewards is similar to discounting money in the case of interest.\n",
        "\n",
        "To compute the expected cumulative reward, known as the **return**, at a given timestep in a learning episode, we sum the discounted rewards expected at that time step $t$, within a learning episode, and projecting into the future. We define the return (cumulative reward) at a time step $t$, $R_{t}$ as:\n",
        "\n",
        ">$R_{t}=\\sum_{k=0}^\\infty\\gamma^kr_{t+k}$\n",
        "\n",
        "where  $0 < \\gamma < 1$ is the discount factor and $r_{t}$ is the reward at time step $t$, and the index $k$ increments projection into the future within a single learning episode. Intuitively, you can think of this function as depreciating any rewards received at later time steps, which will force the agent prioritize getting rewards now. Since we can't extend episodes to infinity, in practice the computation will be limited to the number of timesteps in an episode -- after that the reward is assumed to be zero.\n",
        "\n",
        "Take note of the form of this sum -- we'll have to be clever about how we implement this function. Specifically, we'll need to initialize an array of zeros, with length of the number of time steps, and fill it with the real discounted reward values as we loop through the rewards from the episode, which will have been saved in the agents memory. What we ultimately care about is which actions are better relative to other actions taken in that episode -- so, we'll normalize our computed rewards, using the mean and standard deviation of the rewards across the learning episode.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_Q2OFYtQ32X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Reward function ###\n",
        "\n",
        "# Helper function that normalizes an np.array x\n",
        "def normalize(x):\n",
        "  x -= np.mean(x)\n",
        "  x /= np.std(x)\n",
        "  return x.astype(np.float32)\n",
        "\n",
        "# Compute normalized, discounted, cumulative rewards (i.e., return)\n",
        "# Arguments:\n",
        "#   rewards: reward at timesteps in episode\n",
        "#   gamma: discounting factor\n",
        "# Returns:\n",
        "#   normalized discounted reward\n",
        "def discount_rewards(rewards, gamma=0.95): \n",
        "  discounted_rewards = np.zeros_like(rewards)\n",
        "  R = 0\n",
        "  for t in reversed(range(0, len(rewards))):\n",
        "      # update the total discounted reward\n",
        "      R = R * gamma + rewards[t]\n",
        "      discounted_rewards[t] = R\n",
        "      \n",
        "  return normalize(discounted_rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzbY-mjGYcmt",
        "colab_type": "text"
      },
      "source": [
        "## 3.4 Learning algorithm\n",
        "\n",
        "Now we can start to define the learing algorithm which will be used to reinforce good behaviors of the agent and discourage bad behaviours. In this lab, we will focus on *policy gradient* methods which aim to **maximize** the likelihood of actions that result in large rewards. Equivalently, this means that we want to **minimize** the negative likelihood of these same actions. We achieve this by simply **scaling** the probabilities by their associated rewards -- effectively amplifying the likelihood of actions that resujlt in large rewards.\n",
        "\n",
        "Since the log function is monotonically increasing, this means that minimizing **negative likelihood** is equivalent to minimizing **negative log-likelihood**.  Recall that we can easily compute the negative log-likelihood of a discrete action by evaluting its [softmax cross entropy](https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits). Like in supervised learning, we can use stochastic gradient descent methods to achieve the desired minimization. \n",
        "\n",
        "Let's begin by defining the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsgZ3IDCY_Zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Loss function ###\n",
        "\n",
        "# Arguments:\n",
        "#   logits: network's predictions for actions to take\n",
        "#   actions: the actions the agent took in an episode\n",
        "#   rewards: the rewards the agent received in an episode\n",
        "# Returns:\n",
        "#   loss\n",
        "def compute_loss(logits, actions, rewards): \n",
        "  '''TODO: complete the function call to compute the negative log probabilities'''\n",
        "  neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=actions)\n",
        "  \n",
        "  '''TODO: scale the negative log probability by the rewards'''\n",
        "  loss = tf.reduce_mean(rewards*neg_logprob)\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr5vQ9fqbPpp",
        "colab_type": "text"
      },
      "source": [
        "Now let's use the loss function to define a training step of our learning algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_50ada7nbZ7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Training step (forward and backpropagation) ###\n",
        "\n",
        "def train_step(model, optimizer, observations, actions, discounted_rewards):\n",
        "  with tf.GradientTape() as tape:\n",
        "      # Forward propagate through the agent network\n",
        "      logits = model(observations)\n",
        "\n",
        "      '''TODO: call the compute_loss function to compute the loss'''\n",
        "      loss = compute_loss(logits, actions, discounted_rewards)\n",
        "\n",
        "  '''TODO: run backpropagation to minimize the loss using the tape.gradient method.\n",
        "      Use `model.trainable_variables`'''\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJiqbFYpgYRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Define the Pong agent ###\n",
        "\n",
        "# Functionally define layers for convenience\n",
        "# All convolutional layers will have ReLu activation\n",
        "Conv2D = functools.partial(tf.keras.layers.Conv2D, padding='same', activation='relu')\n",
        "Flatten = tf.keras.layers.Flatten\n",
        "Dense = tf.keras.layers.Dense\n",
        "\n",
        "# Defines a CNN for the Pong agent\n",
        "def create_pong_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "    # Convolutional layers\n",
        "    # First, 16 7x7 filters with 4x4 stride\n",
        "    Conv2D(filters=16, kernel_size=8, strides=4),\n",
        "\n",
        "    # TODO: define convolutional layers with 32 5x5 filters and 2x2 stride\n",
        "    Conv2D(filters=32, kernel_size=4, strides=2),\n",
        "\n",
        "    # TODO: define convolutional layers with 48 3x3 filters and 2x2 stride\n",
        "    Conv2D(filters=48, kernel_size=3, strides=2),\n",
        "\n",
        "    Flatten(),\n",
        "    \n",
        "    # Fully connected layer and output\n",
        "    Dense(units=256, activation='relu'),\n",
        "    # TODO: define the output dimension of the last Dense layer. \n",
        "    # Pay attention to the space the agent needs to act in\n",
        "    Dense(units=6, activation='sigmoid')\n",
        "  \n",
        "  ])\n",
        "  return model\n",
        "\n",
        "pong_model = create_pong_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0RvqOVkmc2r",
        "colab_type": "text"
      },
      "source": [
        "## 3.8 Pong-specific functions\n",
        "\n",
        "In Part 1 (Cartpole), we implemented some key functions and classes to build and train our RL agent -- `choose_action(model, observation)` and the `Memory` class, for example. However, in getting ready to apply these to a new game like Pong, we might need to make some slight modifications. \n",
        "\n",
        "Namely, we need to think about what happens when a game ends. In Pong, we know a game has ended if the reward is +1 (we won!) or -1 (we lost unfortunately). Otherwise, we expect the reward at a timestep to be zero -- the players (or agents) are just playing eachother. So, after a game ends, we will need to reset the reward to zero when a game ends. This will result in a modified reward function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEZG2o50luLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Pong reward function ###\n",
        "\n",
        "# Compute normalized, discounted rewards for Pong (i.e., return)\n",
        "# Arguments:\n",
        "#   rewards: reward at timesteps in episode\n",
        "#   gamma: discounting factor. Note increase to 0.99 -- rate of depreciation will be slower.\n",
        "# Returns:\n",
        "#   normalized discounted reward\n",
        "def discount_rewards(rewards, gamma=0.99): \n",
        "  discounted_rewards = np.zeros_like(rewards)\n",
        "  R = 0\n",
        "  for t in reversed(range(0, len(rewards))):\n",
        "      # NEW: Reset the sum if the reward is not 0 (the game has ended!)\n",
        "      if rewards[t] != 0:\n",
        "        # print('rewards resetted')\n",
        "        R = 0\n",
        "      # update the total discounted reward as before\n",
        "      R = R * gamma + rewards[t]\n",
        "      discounted_rewards[t] = R\n",
        "      # print('rewards adding')\n",
        "      \n",
        "  return normalize(discounted_rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HopLpb4IoOqA",
        "colab_type": "text"
      },
      "source": [
        "Additionally, we have to consider the nature of the observations in the Pong environment, and how they will be fed into our network. Our observations in this case are images. Before we input an image into our network, we'll do a bit of pre-processing to crop and scale, clean up the background colors to a single color, and set the important game elements to a single color. Let's use this function to visualize what an observation might look like before and after pre-processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no5IIYtFm8pI",
        "colab_type": "code",
        "outputId": "42c2566b-2d67-4130-979e-cad03fd0deda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "f = plt.figure(figsize=(10,3))\n",
        "ax = f.add_subplot(121)\n",
        "ax2 = f.add_subplot(122)\n",
        "observation = env.reset()\n",
        "ax.imshow(observation); ax.grid(False);\n",
        "\n",
        "for i in range(30):\n",
        "  observation, _,_,_ = env.step(0)\n",
        "observation_pp0 = mdl.lab3.preprocess_pong(observation)\n",
        "for i in range(30):\n",
        "  # observation, _,_,_ = env.step(4)\n",
        "  observation, _,_,_ = env.step(5)\n",
        "observation_pp1 = mdl.lab3.preprocess_pong(observation)\n",
        "\n",
        "\n",
        "ax2.imshow(np.squeeze(observation_pp1-observation_pp0)); ax2.grid(False); plt.title('Preprocessed Observation0');\n",
        "# ax2.imshow(np.squeeze(observation_pp1)); ax2.grid(False); plt.title('Preprocessed Observation1');"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAADSCAYAAABuBDhNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x216 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYwIWC-Cz8F2",
        "colab_type": "text"
      },
      "source": [
        "What do you notice? How might these changes be important for training our RL algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRqcaDQ1pm3x",
        "colab_type": "text"
      },
      "source": [
        "## 3.9 Training Pong\n",
        "\n",
        "We're now all set up to start training our RL algorithm and agent for the game of Pong! We've already defined our loss function with `compute_loss`, which employs policy gradient learning, as well as our backpropagation step with `train_step` which is beautiful! We will use these functions to execute training the Pong agent. Let's walk through the training block.\n",
        "\n",
        "In Pong, rather than feeding our network one image at a time, it can actually improve performance to input the difference between two consecutive observations, which really gives us information about the movement between frames -- how the game is changing. We'll first pre-process the raw observation, `x`, and then we'll compute the difference with the image frame we saw one timestep before. \n",
        "\n",
        "This observation change will be forward propagated through our Pong agent, the CNN network model, which will then predict the next action to take based on this observation. The raw reward will be computed, and the observation, action, and reward will be recorded into memory. This will continue until a training episode, i.e., a game, ends.\n",
        "\n",
        "Then, we will compute the discounted rewards, and use this information to execute a training step. Memory will be cleared, and we will do it all over again!\n",
        "\n",
        "Let's run the code block to train our Pong agent. Note that completing training will take quite a bit of time (estimated at least a couple of hours). We will again visualize the evolution of the total reward as a function of training to get a sense of how the agent is learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCwyQQrPnkZG",
        "colab_type": "code",
        "outputId": "26507f5f-b8e3-475c-9716-a9c561883949",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "### Training Pong ###\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate=1e-4\n",
        "MAX_ITERS = 10000 # increase the maximum number of episodes, since Pong is more complex!\n",
        "\n",
        "# Model and optimizer\n",
        "pong_model = create_pong_model()\n",
        "obs = env.reset()\n",
        "p_f = mdl.lab3.preprocess_pong(obs)\n",
        "action = choose_action(pong_model,p_f)\n",
        "# pong_model.load_weights('model13.h5')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# plotting\n",
        "smoothed_reward = mdl.util.LossHistory(smoothing_factor=0.9)\n",
        "plotter = mdl.util.PeriodicPlotter(sec=5, xlabel='Iterations', ylabel='Rewards')\n",
        "memory = Memory()\n",
        "\n",
        "for i_episode in range(MAX_ITERS):\n",
        "\n",
        "  plotter.plot(smoothed_reward.get())\n",
        "\n",
        "  # Restart the environment\n",
        "  observation = env.reset()\n",
        "  previous_frame = mdl.lab3.preprocess_pong(observation)\n",
        "\n",
        "  while True:\n",
        "      # Pre-process image \n",
        "      current_frame = mdl.lab3.preprocess_pong(observation)\n",
        "      \n",
        "      '''TODO: determine the observation change\n",
        "      Hint: this is the difference between the past two frames'''\n",
        "      obs_change = current_frame-previous_frame\n",
        "      # f = plt.figure(figsize=(10,3))\n",
        "      # ax = f.add_subplot(121)\n",
        "      # ax2 = f.add_subplot(122)\n",
        "      # # ax.imshow(current_frame); ax.grid(False);\n",
        "      # ax2.imshow(np.squeeze(obs_change)); ax2.grid(False);\n",
        "\n",
        "      \n",
        "      '''TODO: choose an action for the pong model, using the frame difference, and evaluate'''\n",
        "      action = choose_action(pong_model,obs_change)\n",
        "      # print('action is ',obs_change.shape)\n",
        "      # Take the chosen action\n",
        "      next_observation, reward, done, info = env.step(action)\n",
        "\n",
        "      '''TODO: save the observed frame difference, the action that was taken, and the resulting reward!'''\n",
        "      memory.add_to_memory(obs_change, action, reward)\n",
        "      \n",
        "      # is the episode over? did you crash or do so well that you're done?\n",
        "      if done:\n",
        "          # determine total reward and keep a record of this\n",
        "          total_reward = sum(memory.rewards)\n",
        "          smoothed_reward.append( total_reward )\n",
        "\n",
        "          # begin training\n",
        "          train_step(pong_model, \n",
        "                     optimizer, \n",
        "                     observations = np.stack(memory.observations, 0), \n",
        "                     actions = np.array(memory.actions),\n",
        "                     discounted_rewards = discount_rewards(memory.rewards))\n",
        "          \n",
        "          memory.clear()\n",
        "          break\n",
        "\n",
        "      observation = next_observation\n",
        "      previous_frame = current_frame  \n",
        "  if (i_episode%25==0):\n",
        "    saved_pong = mdl.lab3.save_video_of_model(\n",
        "        pong_model, \"Pong-v0\", obs_diff=True, \n",
        "        pp_fn=mdl.lab3.preprocess_pong)\n",
        "    pong_model.save('model_new1.h5')      \n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAEGCAYAAAC+fkgiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnHMu3/w/jvatf07tryEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIlwIgBP3Js6",
        "colab_type": "text"
      },
      "source": [
        "## 3.10 Conclusion\n",
        "\n",
        "That's it! Congratulations on training two RL agents and putting them to the test! We encourage you to consider the following:\n",
        "\n",
        "*   How does the agent perform?\n",
        "*   Could you train it for shorter amounts of time  and still perform well?\n",
        "*   Do you think that training longer would help even more? \n",
        "* How does the complexity of Pong relative to Cartpole alter the rate at which the agent learns and its performance? \n",
        "* What are some things you could change about the agent or the learning process to potentially improve performance?\n",
        "\n",
        "If you want to go further, try to optimize your model to achieve the best performance! **[Email us](mailto:introtodeeplearning-staff@mit.edu) a copy of your notebook with the Pong training executed AND a saved video of your Pong agent competing! We'll give out prizes to the best performers!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKgLyC2ZhXKR",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}